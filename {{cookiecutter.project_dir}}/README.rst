{{cookiecutter.project_name}}
=============================

This Azure Machine Learning project was generated using `Cookiecutter`_ along
with `@wmeints`_'s `azureml-cookiecutter`_ template.

We'll cover the following topics in this README:

.. contents::

Getting started
---------------
To set up your project, follow these steps:

- :code:`conda env create -f conda_dependencies.yml` -n {{cookiecutter.package_name}}
- :code:`conda activate {{cookiecutter.package_name}}`
- :code:`python tasks/make_workspace.py --name <my_workspace> --resource_group <my_resource_group>`
- :code:`python tasks/make_dataset.py --name <dataset_name> --input_file <my_data_file>`
- :code:`python tasks/make_environment.py --name <name>`

After executing these tasks you can start working on your project.

Documentation
-------------

Creating or editing tasks
~~~~~~~~~~~~~~~~~~~~~~~~~
The project contains a number of tasks. Tasks are implemented as python scripts
and allow you to manage your project. For example, tasks allow you to manage the
machine learning workspace, run an experiment on a remote environment, and 
deploy your model to production. 

Check out the predefined tasks to get an idea of what they do and how to create
your own tasks in the project.

Writing your model code
~~~~~~~~~~~~~~~~~~~~~~~
Next to the tasks, there's the code that you use to train your model or perform
inference using your model. These files are stored in the 
:code:`{{cookiecutter.package_name}}` folder.

The following files are created by default:

- :code:`train.py` - This file is used for training the model
- :code:`score.py` - This file is used for inferencing

Working with data
~~~~~~~~~~~~~~~~~
We recommend you split data preprocessing from training as much as 
possible. We also recommend that you split your data preprocessing in several 
sub-tasks each with its own script in the :code`tasks` folder. This will allow 
you to repeat a step when something fails.

Fro that reason, the project contains a :code:`data` folder. This folder is 
split into three sub-folders: :code:`raw`, :code:`interim`, and 
:code:`processed`. You can use these folders to store raw, interim, and fully 
pre-processed datasets.

Use the :code:`tasks/make_dataset.py` script to upload the datasets to the 
azure workspace after you've created. Alternatively, you can write your own
custom task to process data and upload it from there.

Using notebooks
~~~~~~~~~~~~~~~
This repository contains a folder called notebooks. You can add your Python
notebooks to this folder. Use them as your scratch space to explore data.

We encourage you to use scripts instead of the notebooks for any production code.
Notebooks have several limitations that will hurt you in the long run:

- The order is determined by how you execute the cells. It doesn't have 
  to be in the order of appearance inside the notebook file.
- You can't run notebooks from the command line directly.
- It's hard to merge the contents of a notebook should your run into 
  merge-conflicts in your source control environment.

Notebooks are great for exploring data and visualizing things. So we feel that
they still have a place in this template.

Generating reports
~~~~~~~~~~~~~~~~~~
You can use the reports folder to store any generated reports, such as reports
generated by the :code:`pandas-profiling` package.

We recommend generating reports to visualize the performance of your models,
explain outcomes, or explore data.

Generating Documentation
~~~~~~~~~~~~~~~~~~~~~~~~
To make documentation easier, we've included Sphinx docs in the project.
You can generate the HTML documentation using the following command:

  cd docs
  make html

This command works on Windows, Mac, and Linux. 

Please refer to the `Sphinx documentation`_ to learn more about writing rich
documentation based on your code and custom restructured text documents.

Testing your code
~~~~~~~~~~~~~~~~~
It's highly recommended to write automated tests. You can use :code:`pytest` to run unit-tests.
We recommend placing the test code in a folder called :code:`tests` in the root of the project.
This isolates the tests from the rest of the project.

We recommend installing the project using :code:`pip` in editable mode by running the following command in the root of the project:

  pip install -e .

Please note that if you changed the dependencies file, you'll also need to update setup.py to include the new dependencies there as well.
We understand that this can feel redundant, but there currently isn't a way to automate the process.

.. _`Cookiecutter`: https://github.com/audrey/cookiecutter/
.. _`@wmeints`: https://github.com/wmeints/
.. _`azureml-cookiecutter`: https://github.com/wmeints/azureml-cookiecutter/
.. _`Sphinx documentation`: https://www.sphinx-doc.org/en/master/
